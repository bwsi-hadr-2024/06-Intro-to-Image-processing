{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing\n",
    "This lesson will go over how to work with image data, extract information, and perform basic operations and transformations on images.\n",
    "\n",
    "We'll be using primarily the [Pillow](https://pillow.readthedocs.io/en/stable/index.html) and [opencv](https://opencv.org/) libraries.\n",
    "\n",
    "Pillow is a fork of the older, no-longer maintained PIL (python imaging library) package. It provides base classes for representing image data, and some convenience helper functions and dictionaries.\n",
    "\n",
    "OpenCV is a powerful image processing library. It is originally a C++ library, but we will be using the python bindings to call it in python. One important quirk of OpenCV is that **it represents image channels in BGR**, i.e. the first channel is Blue, whereas most other image representations (including PIL) use RGB, where the first channel is Red. Thus, you must make sure to switch up the order of channels when converting between PIL and opencv\n",
    "(_The reason that opencv uses BGR is that old windows systems used to represent images in BGR, and when opencv was created, they followed that convention_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL # Python Image Library (pillow)\n",
    "from PIL.ExifTags import TAGS, GPSTAGS # dictionaries for exiftags and gpstags names\n",
    "import cv2\n",
    "import pathlib # standard library package for working with file paths\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://files.bwsi-remote-sensing.net/data/photos/BWSI-2019-MATF1-photos.zip -O photos.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip -o photos.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pathlib Path object\n",
    "# pathlib provides functions that make it easier to operate with file paths\n",
    "image_folder = pathlib.Path('BWSI-2019-MATF1-photos/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterdir gives an iterator for all of the items within a pathlib Path directory\n",
    "# get list of files that are jpg or png\n",
    "compatible_imgs = [x for x in image_folder.iterdir() \n",
    "                   if ((x.suffix.lower() == '.jpg') \n",
    "                       or (x.suffix.lower() == '.png'))]\n",
    "\n",
    "compatible_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_path = np.random.choice(compatible_imgs)\n",
    "random_img = PIL.Image.open(random_image_path)\n",
    "print(random_image_path)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.imshow(random_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_img_exif = random_img._getexif()\n",
    "random_img_exif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing EXIF\n",
    "The dictionary has numeric codes for keys. For human readability, it's convenient to use the text descriptions as the keys.\n",
    "\n",
    "There are a few lookup dictionaries that can be used to find the text descriptions. We've imported them as TAGS and GPSTAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPSTAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here's some functions to parse the exif data and convert them to decimal format\n",
    "def parse_exif(exif_dict):\n",
    "    human_readable = {}\n",
    "    for k, v in exif_dict.items():\n",
    "        text_key = TAGS[k]\n",
    "        if type(v) == tuple and len(v)==2:\n",
    "            v = divide_tuple(v)\n",
    "        if type(v) == bytes:\n",
    "            v = v.hex()\n",
    "        if k == 34853: # GPS info field\n",
    "            human_readable[text_key] = parse_gps(v)\n",
    "        elif not ((text_key == 'UserComment') or (text_key == 'MakerNote')):\n",
    "            human_readable[text_key] = v\n",
    "    return human_readable\n",
    "      \n",
    "def parse_gps(gps_dict):\n",
    "    readable_gps = {}\n",
    "    for k,v in gps_dict.items():\n",
    "        text_key = GPSTAGS[k]\n",
    "        if text_key in ['GPSLatitude', 'GPSLongitude']:\n",
    "            print(v)\n",
    "            v = convert_GPS_coord(v)\n",
    "        if type(v) == bytes:\n",
    "            v = v.hex()\n",
    "        readable_gps[text_key] = v\n",
    "    return readable_gps\n",
    "\n",
    "def divide_tuple(tup):\n",
    "    try:\n",
    "        quot = tup[0]/tup[1]\n",
    "        return quot\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def convert_GPS_coord(GPS_tuple):\n",
    "    \"\"\" Converts the GPS coordinate given by exif into decimal coord\n",
    "    GPS_Tuple = (\n",
    "       degrees, minutes, seconds\n",
    "       )\n",
    "    \"\"\"\n",
    "    try:\n",
    "        degs = float(GPS_tuple[0])\n",
    "        mins = float(GPS_tuple[1])\n",
    "        secs = float(GPS_tuple[2])\n",
    "        return degs + mins/60 + secs/3600\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parsed = parse_exif(random_img_exif) \n",
    "parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for img_path in compatible_imgs:\n",
    "    img = PIL.Image.open(img_path)\n",
    "    exif = parse_exif(img._getexif())\n",
    "    if 'GPSInfo' in exif.keys():\n",
    "        print(img_path)\n",
    "        print(exif['GPSInfo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Create a dataframe that holds the metadata of the images in the course folder. You do not need to store all of the metadata fields, just the ones that you may be interested in.\n",
    "\n",
    "Note that not all images will have all of the EXIF fields filled out, so you will have entries with blank or placeholder values in certain columns.\n",
    "\n",
    "Note also that the GPSInfo element of EXIF is a dictionary in and of itself. Be cognizant of this when trying to access those attributes. For example, you cannot get exif_info['GPSLatitude'] directly. You have to get it as exif_info['GPSInfo']['GPSLatitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Spaces\n",
    "Colors can be represented as a multi-dimensional space. One common way of representing colors is with RGB values. One can think of RGB as a 3d Cartesian coordinate system, where the R, G, and B values represent the x, y, and z coordinates respectively. A wide array of colors can then be expressed as tuples in this RGB space.\n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/RGB_Cube_Show_lowgamma_cutout_a.png/400px-RGB_Cube_Show_lowgamma_cutout_a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw image\n",
    "raw_img = cv2.imread(str(random_image_path),-1)\n",
    "# Convert to lower resolution for analysis speedup\n",
    "lowres_dim = 1800\n",
    "img_shape = raw_img.shape\n",
    "img = cv2.resize(raw_img,\n",
    "                (int((img_shape[1]/img_shape[0])*lowres_dim), lowres_dim ),\n",
    "                interpolation=cv2.INTER_CUBIC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors look weird if you use imshow on the regular cv image\n",
    "# this is because it's in BGR instead of RGB\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.imshow(img, interpolation = 'bicubic')\n",
    "plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to different color spaces from BGR (cv convention vs. plt convention)\n",
    "RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "grey_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "HSV_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct color space\n",
    "fig = plt.figure(figsize=[10,10])\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.imshow(RGB_img)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the RGB color channels\n",
    "red_channel = RGB_img[:,:,0]\n",
    "green_channel = RGB_img[:,:,1]\n",
    "blue_channel = RGB_img[:,:,2]\n",
    "fig = plt.figure(figsize=[10,15])\n",
    "ax1 = fig.add_subplot(3,1,1)\n",
    "ax1.imshow(red_channel,cmap='Reds')\n",
    "ax1.set_axis_off()\n",
    "ax2 = fig.add_subplot(3,1,2)\n",
    "ax2.imshow(green_channel,cmap='Greens')\n",
    "ax2.set_axis_off()\n",
    "ax3 = fig.add_subplot(3,1,3)\n",
    "ax3.imshow(blue_channel,cmap='Blues')\n",
    "ax3.set_axis_off()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing histograms\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(3,1,1)\n",
    "ax1.hist(red_channel.flatten(),256,[0,256], color = 'r')\n",
    "ax2 = fig.add_subplot(3,1,2)\n",
    "ax2.hist(green_channel.flatten(),256,[0,256], color = 'g')\n",
    "ax3 = fig.add_subplot(3,1,3)\n",
    "ax3.hist(blue_channel.flatten(),256,[0,256], color = 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other color spaces\n",
    "Much like how space can be represented both using cartesian coordinates and other coordinate systems, color spaces can be represented with other coordinates as well.\n",
    "\n",
    "One such alternative is the hue, saturation, value (HSV) system, which can be thought of as a cylindrical coordinate system.\n",
    "- Hue represents the color (low=violet, high=red)\n",
    "- Saturation represents the color intensity (low=grey, high=colorful)\n",
    "- Value represents brightness (low=black, high=white)\n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/3/33/HSV_color_solid_cylinder_saturation_gray.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "For each line of the following lines, explain what is happening using comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_channel = HSV_img[:,:,0]\n",
    "saturation_channel = HSV_img[:,:,1]\n",
    "value_channel = HSV_img[:,:,2]\n",
    "fig = plt.figure(figsize=[10,15])\n",
    "ax1 = fig.add_subplot(3,1,1)\n",
    "ax1.imshow(hue_channel,cmap='Greys_r')\n",
    "ax1.set_axis_off()\n",
    "ax1.set_title('hue')\n",
    "ax2 = fig.add_subplot(3,1,2)\n",
    "ax2.imshow(saturation_channel,cmap='Greys_r')\n",
    "ax2.set_axis_off()\n",
    "ax2.set_title('saturation')\n",
    "ax3 = fig.add_subplot(3,1,3)\n",
    "ax3.imshow(value_channel,cmap='Greys_r')\n",
    "ax3.set_axis_off()\n",
    "ax3.set_title('value')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyplot's imshow assumes that the first channel is red, the second is green, and the last is blue. Thus, we can present other information in the R, G, B channels. For example, we can plot the HSV channels in the RGB channels to get a trippy picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.imshow(HSV_img)# hue -> red, saturation -> green, value -> blue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution filters\n",
    "The process of convolution involves taking the dot product between a \"kernel\" matrix and subsets of a matrix centered at various points across the image, and storing the result of the matrix multiplication in a new matrix as the result of the operation.\n",
    "\n",
    "![convolution_example](https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif)\n",
    "\n",
    "Different types of kernels result in different types of outputs. \n",
    "For this input image:\n",
    "\n",
    "![input image](https://upload.wikimedia.org/wikipedia/commons/5/50/Vd-Orig.png)\n",
    "\n",
    "Here are some types of kernels that can be applied and their effects\n",
    "\n",
    "### Identity\n",
    "![identity kernel](https://wikimedia.org/api/rest_v1/media/math/render/svg/1fbc763a0af339e3a3ff20af60a8a993c53086a7)\n",
    "\n",
    "![input image](https://upload.wikimedia.org/wikipedia/commons/5/50/Vd-Orig.png)\n",
    "### Edge detection\n",
    "![edge_kernel](https://wikimedia.org/api/rest_v1/media/math/render/svg/f9de5913c98629f30efb20b8868e096f202b626c)\n",
    "\n",
    "![edge_result](https://upload.wikimedia.org/wikipedia/commons/2/20/Vd-Rige1.png)\n",
    "### Sharpen\n",
    "![sharpen_kernel](https://wikimedia.org/api/rest_v1/media/math/render/svg/beb8b9a493e8b9cf5deccd61bd845a59ea2e62cc)\n",
    "\n",
    "![sharpen_result](https://upload.wikimedia.org/wikipedia/commons/4/4e/Vd-Sharp.png)\n",
    "### Box Blur \n",
    "![box_blur_kernel](https://wikimedia.org/api/rest_v1/media/math/render/svg/91256bfeece3344f8602e288d445e6422c8b8a1c)\n",
    "\n",
    "![box blur result](https://upload.wikimedia.org/wikipedia/commons/0/04/Vd-Blur2.png)\n",
    "### Gaussian blur (3x3) \n",
    "![gauss kernel 3x3](https://wikimedia.org/api/rest_v1/media/math/render/svg/ca9c0da52fe7818783942b06aac9cf396ae628bf)\n",
    "\n",
    "![gauss result 3x3](https://upload.wikimedia.org/wikipedia/commons/2/28/Vd-Blur1.png)\n",
    "### Gaussian blur (5x5) \n",
    "![gauss kernel 5x5](https://wikimedia.org/api/rest_v1/media/math/render/svg/f91401a3e97428f14862afa1c781c55f4157580b)\n",
    "\n",
    "![gauss result 5x5](https://upload.wikimedia.org/wikipedia/commons/0/04/Vd-Blur_Gaussian_5x5.png)\n",
    "\n",
    "## Convolution Terminology\n",
    "- kernel size: the dimensions of the kernel matrix\n",
    "- stride: how many elements you move over each time you slide the window\n",
    "- padding: how do you deal with the values at the boundary when the kernel \"overhangs\" the original matrix\n",
    "    - some strategies: pad with fixed value (often zero), reflect, wrap, only use valid convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV convolution support\n",
    "OpenCV supports convolutions with arbitrary convolution kernels via the [`filter2D()` function](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga27c049795ce870216ddfb366086b5a04)\n",
    "\n",
    "Some common convolutions also have dedicated functions for convenience, such as [box blur](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga8c45db9afe636703801b0b2e440fce37), and [gaussian blur](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general convolution\n",
    "# create kernel\n",
    "sharpen_kernel = np.array([[0, -1, 0],\n",
    "                           [-1, 5, -1],\n",
    "                           [0, -1, 0]])\n",
    "# the -1 argument for ddepth in filter2D preserves number of layers in output\n",
    "sharpened_img = cv2.filter2D(RGB_img, -1, sharpen_kernel) \n",
    "\n",
    "fig = plt.figure(figsize=[20,10])\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.imshow(RGB_img)\n",
    "ax.set_axis_off()\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.imshow(sharpened_img)\n",
    "ax2.set_axis_off()\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for gaussian blur\n",
    "gauss_blur_kernel_size = (13,13)\n",
    "gauss_blur_kernel_sigma = 19\n",
    "blurred_RGB_img = cv2.GaussianBlur(RGB_img,\n",
    "                                   gauss_blur_kernel_size,\n",
    "                                   gauss_blur_kernel_sigma)\n",
    "\n",
    "fig = plt.figure(figsize=[20,10])\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.imshow(RGB_img)\n",
    "ax.set_axis_off()\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.imshow(blurred_RGB_img)\n",
    "ax2.set_axis_off()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding\n",
    "Thresholding allows you to convert pixel values that are above/below a threshold to 0 or 1.\n",
    "\n",
    "There are a ton of different [thresholding schemes](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html).\n",
    "\n",
    "There are simple thresholding schemes:\n",
    " - cv2.THRESH_BINARY\n",
    " - cv2.THRESH_BINARY_INV\n",
    " - cv2.THRESH_TRUNC\n",
    " - cv2.THRESH_TOZERO\n",
    " - cv2.THRESH_TOZERO_INV\n",
    "\n",
    "![thresholding_simple](https://docs.opencv.org/3.0-beta/_images/threshold.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original image in greyscale\n",
    "plt.imshow(grey_img, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try a simple thresholding\n",
    "# values from 128-255 get converted to 255\n",
    "# values below 128 get converted to zero\n",
    "ret, grey_thresh = cv2.threshold(grey_img, 128,255, cv2.THRESH_BINARY)\n",
    "plt.imshow(grey_thresh, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive thresholding\n",
    "Simple threshold manually sets the threshold point. There are a number of adaptive methods which use the values of the other pixels to determine the threshold.\n",
    "\n",
    "There are two available adaptive thresholds:\n",
    "- cv2.ADAPTIVE_THRESH_MEAN_C : threshold value is the mean of neighbourhood area.\n",
    "- cv2.ADAPTIVE_THRESH_GAUSSIAN_C : threshold value is the weighted sum of neighbourhood values where weights are a gaussian window.\n",
    "\n",
    "These take two additional arguments:\n",
    "- neighborhood: the number of pixels to use to compute the mean or gaussian\n",
    "- c: a constant that's subtracted from the window\n",
    "\n",
    "These can be tuned to adjust performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using mean\n",
    "grey_thresh = cv2.adaptiveThreshold(grey_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n",
    "                                   cv2.THRESH_BINARY, 301, 5)\n",
    "plt.imshow(grey_thresh, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gaussian\n",
    "grey_thresh = cv2.adaptiveThreshold(grey_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                   cv2.THRESH_BINARY, 501, 10)\n",
    "plt.imshow(grey_thresh, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, grey_thresh = cv2.threshold(grey_img, 0,255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "plt.imshow(grey_thresh, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first applying a gaussian blur can reduce noise in the threshold\n",
    "blurred_img = cv2.GaussianBlur(grey_img,(5,5),0)\n",
    "plt.imshow(blurred_img, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this reduces some of the noise\n",
    "ret, grey_thresh = cv2.threshold(blurred_img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "plt.imshow(grey_thresh, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(img):\n",
    "    # Blur for lowpass\n",
    "    img = cv2.GaussianBlur(img,(11,11),1)\n",
    "    # bilat_filt for further noise reduction w/ edge preservation\n",
    "    img = cv2.bilateralFilter(img,5,20,20)\n",
    "    # Compute edges\n",
    "    edges = cv2.Canny(img,30,60)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_edges(grey_img),cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_edges(grey_thresh),cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_edges(value_channel),cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "Using the image processing techniques learned today, apply them to analyze some satellite or aerial imagery.\n",
    "For example, you can use thresholding to isolate extent of a fire using the Normalized Burn Ratio index, or get the front of the fire using edge detection.\n",
    "\n",
    "There are a ton of additional opencv capabilities beyond what we've covered in class. Check the [documentation](https://docs.opencv.org/4.5.2/d6/d00/tutorial_py_root.html) for more ideas!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
